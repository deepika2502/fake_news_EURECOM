{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7669b20",
   "metadata": {},
   "source": [
    "# Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ce41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing required repositories\n",
    "!pip install sentence-transformers\n",
    "!sudo apt install tesseract-ocr\n",
    "!pip3 install pytesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad343fcf",
   "metadata": {},
   "source": [
    "# Import Necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa7cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms as T\n",
    "from torchvision.datasets import FakeData\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Softmax\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e4de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ba12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion() \n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47c278d",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec1c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  'data.csv'\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e145ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77669f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['2_way_label'].unique())\n",
    "print(df['3_way_label'].unique())\n",
    "print(df['6_way_label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7326a07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class 0 in 2 way: \"+ str(sum(df['2_way_label'] == 0)))\n",
    "print(\"Class 1 in 2 way: \"+ str(sum(df['2_way_label'] == 1)))\n",
    "print(\"\\n\\nClass 0 in 3 way: \"+ str(sum(df['3_way_label'] == 0)))\n",
    "print(\"Class 1 in 3 way: \"+ str(sum(df['3_way_label'] == 1)))\n",
    "print(\"Class 2 in 3 way: \"+ str(sum(df['3_way_label'] == 2)))\n",
    "print(\"\\n\\nClass 0 in 6 way: \"+ str(sum(df['6_way_label'] == 0)))\n",
    "print(\"Class 1 in 6 way: \"+ str(sum(df['6_way_label'] == 1)))\n",
    "print(\"Class 2 in 6 way: \"+ str(sum(df['6_way_label'] == 2)))\n",
    "print(\"Class 3 in 6 way: \"+ str(sum(df['6_way_label'] == 3)))\n",
    "print(\"Class 4 in 6 way: \"+ str(sum(df['6_way_label'] == 4)))\n",
    "print(\"Class 5 in 6 way: \"+ str(sum(df['6_way_label'] == 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046fa199",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hasImage'].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab273853",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_title'].isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d052c35",
   "metadata": {},
   "source": [
    "# Sentence BERT Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e5cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import model for SBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sentence_model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ac6e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the sentences to extract embeddings\n",
    "sentence = list(df['clean_title'])\n",
    "sentence = sentence_model.encode(sentence)\n",
    "print(sentence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d320201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_embeddings = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a11324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not neccesary to include this, but the class definition has transform mentioned, hence it is added\n",
    "transform = T.Compose([T.Resize(256), T.CenterCrop(224), T.ToTensor(), \n",
    "                       T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0b4053",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(combined_embeddings), len(combined_embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4a6894",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_embeddings = np.asarray(combined_embeddings)\n",
    "combined_embeddings = torch.tensor(np.array([combined_embeddings]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863eb305",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_embeddings.shape)\n",
    "print(combined_embeddings.shape[2])\n",
    "print(combined_embeddings.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf28d5",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424ae2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSentenceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_path, embedding, transform = transforms.ToTensor()):\n",
    "        self.csv_path = csv_path\n",
    "        self.transform = transform\n",
    "        self.embeddings = embedding\n",
    "        self.labels = pd.read_csv(csv_path).iloc[:, 15]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #Sentence\n",
    "        label = self.labels[index]\n",
    "        embedding = self.embeddings[index]\n",
    "        return (embedding, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3326d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  'data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0714f2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset_from_csv = ImageSentenceDataset(path, combined_embeddings[0], \n",
    "                                               transform= transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83897895",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(custom_dataset_from_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad49df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Train Val Split\n",
    "train_set, test_set, val_set = torch.utils.data.random_split(custom_dataset_from_csv, [17622-3000, 1500, 1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce90807",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_set), len(test_set), len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb25c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "                                                    batch_size=28,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b9dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just confirm what you see here\n",
    "for em, label in (custom_dataset_loader):\n",
    "  print(em, em.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41913eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define MLP\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 100)\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        self.act1 = ReLU()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(100, 30)\n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        self.act2 = ReLU()\n",
    "        self.hidden3 = Linear(30, 6)\n",
    "        kaiming_uniform_(self.hidden3.weight, nonlinearity='relu')\n",
    "        self.act3 = ReLU()\n",
    "        # third hidden layer and output\n",
    "        self.hidden4 = Linear(6, 6)\n",
    "        xavier_uniform_(self.hidden4.weight)\n",
    "        self.act4 = Softmax(dim=1)\n",
    "\n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "        # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # output layer\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        X = self.hidden4(X)\n",
    "        X = self.act4(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de40f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(combined_embeddings.shape[2]).to(device)\n",
    "# train the model\n",
    "#train_model(custom_dataset_loader, model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), amsgrad = True)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4de2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a training function\n",
    "def train(epoch, log_interval=200):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    for epoch in range(epoch):\n",
    "        # Loop over each batch from the training set\n",
    "        for batch_idx, (data, target) in enumerate(custom_dataset_loader):\n",
    "            # Copy data to GPU if needed\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            # Zero gradient buffers\n",
    "            optimizer.zero_grad() \n",
    "            # Pass data through the network\n",
    "            output = model(data)\n",
    "            # Calculate loss\n",
    "            loss = criterion(output, target)\n",
    "            # Backpropagate\n",
    "            loss.backward()        \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(custom_dataset_loader.dataset),\n",
    "                    100. * batch_idx / len(custom_dataset_loader), loss.data.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d364004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fbc644",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ba340",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation of model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.cpu().detach().numpy()\n",
    "        actual = targets.cpu().numpy()\n",
    "        # convert to class labels\n",
    "        yhat = np.argmax(yhat, axis=1)\n",
    "        # reshape for stacking\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        yhat = yhat.reshape((len(yhat), 1))\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = np.vstack(predictions), np.vstack(actuals)\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    report = classification_report(actuals, predictions)\n",
    "    return acc, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990eb433",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dca7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, report = evaluate_model(test_dataset_loader, model)\n",
    "print(\"Accuracy and classification report\")\n",
    "print(acc)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca9ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_loader = torch.utils.data.DataLoader(dataset=val_set,\n",
    "                                                    batch_size=64,\n",
    "                                                    shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9b8553",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, report = evaluate_model(val_dataset_loader, model)\n",
    "print(\"Accuracy and classification report\")\n",
    "print(acc)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755386e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e5f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f449063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New heading"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
